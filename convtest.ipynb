{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation test using dilated convolutional networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of corpus file (without txt extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpusname = \"lapeceramicro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of past input tokens to use for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputtokens = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network architecture to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "architecture = \"dilatedconv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all relevant file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpusfile = 'corpus/' + corpusname + '.txt'\n",
    "encodername = corpusname + '.enc'\n",
    "modelname = corpusname + '.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from neurowriter.models import modelbyname\n",
    "modelclass = modelbyname(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(corpusfile) as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2/11/2014, 11:54 - Sergio Nabil Khayyat cre√≥ el grupo ‚ÄúLa pecera‚Äù\\n2/11/2014, 11:55 - Sergio Nabil Khayyat te a√±adi√≥\\n2/11/2014, 11:54 - Sergio Nabil Khayyat cambi√≥ el icono de este grupo\\n2/11/2014, 12:04 - Carmen Torrijos cambi√≥ el asunto a ‚ÄúLa pecera \\ue522‚Äù\\n2/11/2014, 12:06 - √Ålvaro Barbero Jim√©nez: Ola k aseis\\n2/11/2014, 12:07 - √Ålvaro Barbero Jim√©nez: Bamonos dE peshka\\n2/11/2014, 12:07 - √Ålvaro Barbero Jim√©nez: Al bershka\\n2/11/2014, 12:07 - Carmen Torrijos: Buena inauguracion del grupo jajaj \\ue00e\\n2/11/2014, 23:33 - Jorge L√≥pez L√°zaro: Capitalism Explained. This Is So Accurate It Hurts. - http://m.tickld.com/x/capitalism-explained-this-is-so-accurate-it-hurts\\n3/11/2014, 0:37 - Alicia Gonz√°lez: \\ue412\\n3/11/2014, 10:06 - Sergio Nabil Khayyat a√±adi√≥ a David D√≠az Vico\\n4/11/2014, 8:34 - Carmen Torrijos: Pastas en el office! üç•üç•\\n4/11/2014, 8:45 - Alicia Gonz√°lez: llego mazo tarde \\ue058\\n4/11/2014, 8:46 - Carmen Torrijos: \\ue115\\ue230\\n4/11/2014, 11:47 - Sergio Nabil Khayyat a√±adi√≥ a Adri√°n Ram√≠rez\\n10/11/2014, 19:08 - S'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:min(1000,len(corpus))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder lapeceramicro.enc\n"
     ]
    }
   ],
   "source": [
    "from neurowriter.encoding import Encoder, loadencoding\n",
    "try:\n",
    "    encoder = loadencoding(encodername)\n",
    "    print(\"Loaded encoder\", encodername)\n",
    "except Exception as e:\n",
    "    print(\"Encoder not found, creating new encoder:\", e)\n",
    "    encoder = Encoder(corpus)\n",
    "    encoder.save(encodername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: [4, 32, 0.60276337607164387, 2, 64, 0.64589411306665612, 'rmsprop'] , loss:  3.60489155451\n",
      "Params: [5, 64, 0.38344151882577771, 3, 64, 0.56804456109393231, 'adam'] , loss:  3.58911536535\n",
      "Params: [2, 4, 0.020218397440325719, 3, 128, 0.87001214824681916, 'adam'] , loss:  3.59204134941\n",
      "Params: [5, 16, 0.78052917628645546, 0, 128, 0.1433532874090464, 'adam'] , loss:  3.73669484456\n",
      "Params: [4, 16, 0.26455561210462697, 3, 64, 0.56843394886864851, 'sgd'] , loss:  3.61186521848\n",
      "Params: [4, 32, 0.61693399687475692, 3, 128, 0.35950790057378601, 'rmsprop'] , loss:  3.61531596184\n",
      "Params: [4, 4, 0.66676671544566768, 2, 32, 0.12892629765485331, 'sgd'] , loss:  3.65036411285\n",
      "Params: [3, 16, 0.43860151346232035, 3, 16, 0.20887675609483469, 'sgd'] , loss:  3.60718886058\n",
      "Params: [4, 8, 0.46631077285630629, 0, 16, 0.11037514116430513, 'rmsprop'] , loss:  3.74341654778\n",
      "Params: [2, 4, 0.36872517066096411, 3, 16, 0.8379449074988039, 'sgd'] , loss:  3.65118765831\n",
      "Params: [4, 64, 0.53074244144301741, 0, 32, 0.94464583293634086, 'rmsprop'] , loss:  3.19233778318\n",
      "Params: [2, 64, 0.77955082506835049, 3, 64, 0.96309199620275521, 'rmsprop'] , loss:  3.59246122042\n",
      "Params: [5, 64, 0.072577230789806602, 1, 32, 0.94568820836463585, 'rmsprop'] , loss:  3.60532447497\n",
      "Params: [2, 64, 0.79835378623829534, 0, 32, 0.71965786019848654, 'sgd'] , loss:  3.79235548973\n",
      "Params: [4, 64, 0.31176238639079479, 1, 32, 0.93707285364491244, 'rmsprop'] , loss:  3.60400276184\n",
      "Params: [4, 64, 0.48306192482051535, 1, 32, 0.94403814350671722, 'rmsprop'] , loss:  3.6058022817\n",
      "Params: [4, 8, 0.78599524691741041, 1, 256, 0.94457584587773813, 'sgd'] , loss:  3.6264113903\n",
      "Params: [4, 64, 0.54552599161559201, 0, 128, 0.96223676057858398, 'rmsprop'] , loss:  3.19415775935\n",
      "Params: [4, 64, 0.5349080534274917, 0, 256, 0.97041124207670926, 'rmsprop'] , loss:  3.21643865903\n",
      "Params: [4, 4, 0.54464226309475139, 0, 32, 0.98915477897325255, 'sgd'] , loss:  3.70475854874\n",
      "Params: [4, 32, 0.50962369341239733, 0, 64, 0.97399184514545878, 'adam'] , loss:  3.10932882627\n",
      "Params: [4, 16, 0.51931454225047913, 0, 16, 0.98770852315483837, 'sgd'] , loss:  3.72991148631\n",
      "Params: [3, 64, 0.11332591350437038, 0, 256, 0.97208125856119254, 'adam'] , loss:  2.91785527865\n",
      "Params: [3, 64, 0.5858744185565441, 0, 256, 0.97273533727141448, 'adam'] , loss:  2.81940960884\n",
      "Params: [3, 16, 0.5731333915693968, 0, 64, 0.99798016213841856, 'adam'] , loss:  2.99940066338\n",
      "Params: [3, 4, 0.70993047942291754, 0, 256, 0.97840236551895232, 'adam'] , loss:  3.62989004453\n",
      "Params: [3, 8, 0.31415612079925992, 0, 64, 0.98551292604024987, 'adam'] , loss:  3.04699713389\n",
      "Params: [3, 64, 0.43779224649845294, 0, 16, 0.97186138776130226, 'adam'] , loss:  2.73048048019\n",
      "Params: [3, 64, 0.26851135315486219, 0, 16, 0.96684593197890201, 'adam'] , loss:  2.80014333725\n",
      "Params: [3, 64, 0.58174981067132137, 0, 16, 0.97101445651032214, 'adam'] , loss:  2.82030480703\n",
      "Params: [3, 64, 0.31105787073272984, 0, 16, 0.95245886683416447, 'adam'] , loss:  3.12627294858\n",
      "Params: [3, 64, 0.55427959070264399, 0, 64, 0.99833969972327774, 'rmsprop'] , loss:  2.97878216108\n",
      "Params: [3, 64, 0.22481704567227412, 0, 128, 0.98667380395876592, 'adam'] , loss:  2.74786520004\n",
      "Params: [3, 64, 0.20815492522892232, 0, 64, 0.98418020710907228, 'adam'] , loss:  2.7282298406\n",
      "Params: [3, 64, 0.58943624946822515, 0, 16, 0.99629113858366214, 'adam'] , loss:  2.75746572812\n",
      "Params: [3, 64, 0.18200909508211061, 0, 64, 0.9907099372317526, 'rmsprop'] , loss:  2.90284053485\n"
     ]
    }
   ],
   "source": [
    "from neurowriter.optimizer import hypertrain\n",
    "\n",
    "model, train_history = hypertrain(modelclass, inputtokens, encoder, corpus, n_calls=100)\n",
    "model.save(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neurowriter.writer import Writer\n",
    "\n",
    "writer = Writer(model, encoder, creativity=0.1)\n",
    "print(corpus[:inputtokens])\n",
    "''.join(writer.write(seed=corpus[:inputtokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Manual test generation test with 0 creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = corpus[:inputtokens]\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Generated\")\n",
    "print(seed, end='')\n",
    "for i in range(1000):\n",
    "    seedcoded = encoder.encodetext(seed)\n",
    "    #cls = model.predict_classes(np.array([seedcoded]), verbose=0)\n",
    "    #char = encoder.index2char[cls[0]]\n",
    "    cls = np.argmax(model.predict(np.array([seedcoded])))\n",
    "    char = encoder.index2char[cls]\n",
    "    print(char, end='')\n",
    "    seed = seed[1:] + char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try training with SGD and the full pecera corpus for a large number of iterations\n",
    "* Add more residual blocks\n",
    "\n",
    "From Facebook's convolutional translation paper\n",
    "* Tokens are dealt with embeddings instead of one-hot encoder.\n",
    "* The position of each token is also added as a parallel embedding\n",
    "* Dropout for the embeddings and for the input of each convolutional block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* WaveNet paper: https://arxiv.org/pdf/1609.03499.pdf\n",
    "* A Keras implementation of WaveNet: https://github.com/usernaamee/keras-wavenet/blob/master/simple-generative-model.py\n",
    "* Another one: https://github.com/basveeling/wavenet/blob/master/wavenet.py\n",
    "* Facebook's convolutional translation paper: https://arxiv.org/pdf/1705.03122.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scrapyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sampletext(logs):\n",
    "    \"\"\"Function that generates some sample text with the model.\n",
    "\n",
    "    Intented to be used as a keras callback\n",
    "    \"\"\"\n",
    "    writer = Writer(model, encoder, creativity=0.1)\n",
    "    print(corpus[:inputtokens])\n",
    "    print(''.join(writer.write(seed=corpus[:inputtokens])))\n",
    "\n",
    "# Build model with input parameters\n",
    "model = modelkind(inputtokens, encoder, *bestparams)\n",
    "# Prepare callbacks\n",
    "callbacks = [\n",
    "    LambdaCallback(on_train_end=sampletext),\n",
    "    ModelCheckpoint(filepath=modelname,save_best_only=True),\n",
    "    EarlyStopping(patience=patience)\n",
    "]\n",
    "# Train model\n",
    "model.fit_generator(\n",
    "    traingenerator,\n",
    "    steps_per_epoch=int((1-val)*(len(corpus)-inputtokens+1)/batchsize),\n",
    "    validation_data=valgenerator,\n",
    "    validation_steps=int(val*(len(corpus)-inputtokens+1)/batchsize),\n",
    "    epochs=maxepochs,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
