{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of text generation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of corpus file (without txt extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpusname = \"toyseries\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of the tokenizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = \"word\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of past input tokens to use for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputtokens = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network architecture to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "architecture = \"wavenet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hyperoptimization trials (recommended at least 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypertrials = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all relevant file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpusfile = 'corpus/' + corpusname + '.txt'\n",
    "encodername = corpusname + '.enc'\n",
    "modelname = corpusname + '.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neurowriter.models import modelbyname\n",
    "modelclass = modelbyname(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(corpusfile) as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus[0:min(1000,len(corpus))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neurowriter.encoding import Encoder\n",
    "encoder = Encoder(corpus, tokenizer)\n",
    "encoder.save(encodername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the generator model, trying different hyperparameters and selecting the model producing lower loss in a  validation split of the data.\n",
    "\n",
    "Note this might take a very long time, so during the optimization temporary versions of the model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neurowriter.optimizer import hypertrain\n",
    "\n",
    "model, train_history = hypertrain(modelclass, inputtokens, encoder, corpus, n_calls=hypertrials, savemodel=modelname)\n",
    "model.save(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neurowriter.writer import Writer\n",
    "\n",
    "writer = Writer(model, encoder, creativity=0.1)\n",
    "tokens = encoder.tokenizer.transform(corpus)\n",
    "seed = tokens[:inputtokens]\n",
    "seedtxt = encoder.tokenizer.intertoken.join(seed)\n",
    "print(seedtxt)\n",
    "''.join(writer.write(seed=seedtxt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try training with SGD and the full pecera corpus for a large number of iterations\n",
    "\n",
    "From Facebook's convolutional translation paper\n",
    "* Tokens are dealt with embeddings instead of one-hot encoder.\n",
    "* The position of each token is also added as a parallel embedding\n",
    "* Dropout for the embeddings and for the input of each convolutional block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* WaveNet paper: https://arxiv.org/pdf/1609.03499.pdf\n",
    "* A Keras implementation of WaveNet: https://github.com/usernaamee/keras-wavenet/blob/master/simple-generative-model.py\n",
    "* Another one: https://github.com/basveeling/wavenet/blob/master/wavenet.py\n",
    "* Facebook's convolutional translation paper: https://arxiv.org/pdf/1705.03122.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scrapyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sampletext(logs):\n",
    "    \"\"\"Function that generates some sample text with the model.\n",
    "\n",
    "    Intented to be used as a keras callback\n",
    "    \"\"\"\n",
    "    writer = Writer(model, encoder, creativity=0.1)\n",
    "    print(corpus[:inputtokens])\n",
    "    print(''.join(writer.write(seed=corpus[:inputtokens])))\n",
    "\n",
    "# Build model with input parameters\n",
    "model = modelkind(inputtokens, encoder, *bestparams)\n",
    "# Prepare callbacks\n",
    "callbacks = [\n",
    "    LambdaCallback(on_train_end=sampletext),\n",
    "    ModelCheckpoint(filepath=modelname,save_best_only=True),\n",
    "    EarlyStopping(patience=patience)\n",
    "]\n",
    "# Train model\n",
    "model.fit_generator(\n",
    "    traingenerator,\n",
    "    steps_per_epoch=int((1-val)*(len(corpus)-inputtokens+1)/batchsize),\n",
    "    validation_data=valgenerator,\n",
    "    validation_steps=int(val*(len(corpus)-inputtokens+1)/batchsize),\n",
    "    epochs=maxepochs,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
